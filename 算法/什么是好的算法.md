🐾 什么是好的算法

🕘 2020.03.17 由 hoanfirst 编辑

原文 《吴军的谷歌方法论》 – 什么是好的计算机算法？对效率影响有多大？


质量的差别，是数量级的。

人本能地对大数没有概念，计算机计算速度非常快，以至于人们对一毫秒和一微秒的差别是无感的，但是计算机要处理的数据量也是非常大的，因此累计起来后者比前者快了一千倍。

另外，很多计算机也对计算机资源的数量没有概念，总觉得是无穷大，因此无端浪费了很多资源。

### 一、衡量标准

既然谈到了算法的好坏，就必须明确衡量算法的标准，以及测试的方法。

什么是好的算法？很多人首先想到“速度快”，也有人想到“占用内存空间不要太大”，这两个标准从大方向本身都没有问题，但**用多少数据来测试算法的速度和空间**却是一个问题，因为对不同数量的数据进行测试，可能算法A在小数据量场景下运行时间快，算法B在大数据量场景下运行时间快。

按照人的思维，可能会说，根据不同场景选用好的算法，然而计算机是比较笨的，不会辩证法。这就要求人们最好制定一个明确的标准，不要一会这样，一会那样....

1965年Juris Hartmanis和Richard Stearns提出了**算法复杂度**的概念，计算机科学家们开始考虑一个公平的、一致的评判算法好坏的方法。

不过，最早将复杂度严格量化衡量的是著名计算机科学家、算法分析之父Gon Knuth / 高德纳。

**Gon Knuth的思想**主要包括三个部分：

- 在比较算法快慢时，需要考虑数据量特别大，甚至近乎无穷大的情况。因为计算机的发明就是为了处理大量数据的...

- 决定算法快慢的因素虽然可能很多，但主要分为两类：不随数据量变化的因素和随数量变化的因素。在研究算法时，不必考虑不变的因素，因为当可变因素趋于无穷大时，不变因素的影响是微乎其微的...而且，任何随着可变因素的变化，通常会造成量级的差异

- 如果两个算法在量级上相当，那么在计算机科学里，就认为它们是一样好的...







